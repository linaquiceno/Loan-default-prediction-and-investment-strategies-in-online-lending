---
title: "assignment 2 question 1"
author: "Lauren Sansone, Joshua Pollack, Lina Quiceno Bejarano"
date: "4/12/2021"
output:
  pdf_document: default
  html_document: default
---

```{r message=FALSE, warning=FALSE, include=FALSE}
#loading all Necessary Libraries
library(tidyverse)
library(lubridate)
library(rpart)
library(rpart.plot)
library(caret)
library(C50)
library('ROCR')
library(ranger)
library(glmnet)
library(xgboost)
library(tidyr)
library(ROSE)
library(broom)
library(rsample)
library(glmnet)
```

```{r message=FALSE, warning=FALSE, include=FALSE}
#Importing the data set
lcDataSample5m <- read_csv("lcDataSample5m.csv")
lcdf_AR <- lcDataSample5m
```

### Variable Modifications

```{r, output.lenght=32}
#Remove loans with a status other than charged off and Fully Paid
lcdf_AR <- lcdf_AR %>% filter(loan_status == "Fully Paid" | loan_status == "Charged Off")

#regrouping purpose
lcdf_AR$purpose <- fct_recode(lcdf_AR$purpose, other="wedding", other="renewable_energy")

#Filtering home ownership
lcdf_AR <- lcdf_AR %>% filter(home_ownership == "MORTGAGE" 
                        | home_ownership == "OWN" 
                        | home_ownership == "RENT")

lcdf_AR <- lcdf_AR %>% mutate_if(is.character, as.factor)

lcdf_AR <- lcdf_AR %>% mutate(loan_status=as.factor(loan_status)) #this is a redundancy

##Building annRet 
lcdf_AR$annRet <- ((lcdf_AR$total_pymnt -lcdf_AR$funded_amnt)/lcdf_AR$funded_amnt)*(12/36)*100
```

### Calculating actual loan returns

```{r message=FALSE, warning=FALSE}

#  First step is to past "01-" to the character string, to get something like "01-Dec-2018", i.e. first of each month 
lcdf_AR$last_pymnt_d<-paste(lcdf_AR$last_pymnt_d, "-01", sep = "")
#     Then convert this character to a date type variable
lcdf_AR$last_pymnt_d<-parse_date_time(lcdf_AR$last_pymnt_d,  "myd")

#Determining Acutal Term or setting term to 3 years
lcdf_AR$actualTerm <- ifelse(lcdf_AR$loan_status=="Fully Paid", as.duration(lcdf_AR$issue_d  %--% lcdf_AR$last_pymnt_d)/dyears(1), 3)

#Then, considering this actual term, the actual annual return is
lcdf_AR$actualReturn <- ifelse(lcdf_AR$actualTerm>0, ((lcdf_AR$total_pymnt -lcdf_AR$funded_amnt)/lcdf_AR$funded_amnt)*(1/lcdf_AR$actualTerm)*100, 0)

```

### Removing Variables due to leakage

```{r, output.lenght=32}
## Removing variables for data leakage
lcdf_AR <- lcdf_AR %>% select(-c(acc_now_delinq, collection_recovery_fee, debt_settlement_flag, debt_settlement_flag_date, deferral_term, delinq_2yrs, disbursement_method, hardship_amount, hardship_dpd, hardship_end_date, hardship_flag, hardship_last_payment_amount,hardship_length, hardship_loan_status, hardship_payoff_balance_amount, hardship_reason, hardship_status, hardship_start_date, hardship_type, inq_last_6mths, issue_d, last_credit_pull_d, last_pymnt_amnt, last_pymnt_d, mths_since_last_delinq, mths_since_last_major_derog, next_pymnt_d, open_acc, orig_projected_additional_accrued_interest, out_prncp, out_prncp_inv, payment_plan_start_date, pub_rec, pymnt_plan, recoveries, revol_bal, revol_util, settlement_date, settlement_amount, settlement_status, settlement_percentage, settlement_term, tot_coll_amt, tot_cur_bal, total_acc, total_pymnt, total_pymnt_inv, total_rec_int, total_rec_late_fee, total_rec_prncp))
                           
```

### Removing variables for other reasons

```{r, output.lenght=32}
## Removing variables for other reasons
lcdf_AR <- lcdf_AR %>% select(-c(addr_state, all_util, annual_inc_joint, application_type, desc, dti_joint, emp_title, funded_amnt, funded_amnt_inv, il_util, inq_fi, inq_last_12m, max_bal_bc, mths_since_last_record, mths_since_rcnt_il, mths_since_recent_bc_dlq, mths_since_recent_revol_delinq, open_acc_6m, open_act_il, open_il_12m, open_il_24m,  open_rv_12m, open_rv_24m, policy_code, revol_bal_joint, sec_app_chargeoff_within_12_mths, sec_app_collections_12_mths_ex_med, sec_app_earliest_cr_line, sec_app_inq_last_6mths, sec_app_mort_acc, sec_app_mths_since_last_major_derog, sec_app_num_rev_accts, sec_app_open_acc, sec_app_open_act_il, sec_app_revol_util, term, title, total_bal_il, total_cu_tl, url, verification_status_joint, zip_code, X1))
```

### Replacing Some Missing Values

```{r, output.lenght=32}
lcdf_AR<- lcdf_AR %>% replace_na(list(mths_since_last_delinq=500, bc_open_to_buy=median(lcdf_AR$bc_open_to_buy, na.rm=TRUE), mo_sin_old_il_acct=1000, mths_since_recent_bc=1000, mths_since_recent_inq=50, num_tl_120dpd_2m = median(lcdf_AR$num_tl_120dpd_2m, na.rm=TRUE), percent_bc_gt_75 = median(lcdf_AR$percent_bc_gt_75, na.rm=TRUE), bc_util=median(lcdf_AR$bc_util, na.rm=TRUE) ))
```

### Removing Variables with \>60% missing values

```{r}
    #remove variables which have more than 60% missing values
nm<-names(lcdf_AR)[colMeans(is.na(lcdf_AR))>0.6]
lcdf_AR <- lcdf_AR %>% select(-nm)

```

### Removing Variables with lots of zeros

```{r, output.lenght=32}
lcdf_AR <- lcdf_AR %>% select(-c(collections_12_mths_ex_med, chargeoff_within_12_mths, delinq_amnt, num_tl_120dpd_2m, num_tl_30dpd, num_tl_90g_dpd_24m, mort_acc, pub_rec_bankruptcies, tax_liens))
```

### Spliting Data into Training, Validation, and Test Sets

```{r}
set.seed(123)

fractionTraining   <- 0.70
fractionValidation <- 0.00
fractionTest       <- 0.30

# Compute sample sizes.
sampleSizeTraining   <- floor(fractionTraining   * nrow(lcdf_AR))
sampleSizeValidation <- floor(fractionValidation * nrow(lcdf_AR))
sampleSizeTest       <- floor(fractionTest       * nrow(lcdf_AR))

# Create the randomly-sampled indices for the dataframe. Use setdiff() to
# avoid overlapping subsets of indices.
indicesTraining    <- sort(sample(seq_len(nrow(lcdf_AR)), size=sampleSizeTraining))
indicesNotTraining <- setdiff(seq_len(nrow(lcdf_AR)), indicesTraining)
indicesValidation  <- sort(sample(indicesNotTraining, size=sampleSizeValidation))
indicesTest        <- setdiff(indicesNotTraining, indicesValidation)

# Finally, output the three dataframes for training, validation and test.
lcdfTrn_AR <- lcdf_AR[indicesTraining, ]
lcdfVal_AR <- lcdf_AR[indicesValidation, ]
lcdfTst_AR <- lcdf_AR[indicesTest, ]

```

## Question 1A

Develop linear (glm) models to predict loan_status. Experiment with different parameter values,and identify which gives 'best' performance. Describe how you determine 'best' performance.How do you handle variable selection?

\#For Ridge and Lasso we experimented with several different values of alpha. Putting alpha at 1, 0.5, and 0 to see how that changed the performance of our models. Plotting the Ridge (left) and 0.5 alpha score (right) we see how their curves based on the log of Lambda follows a completely different path.

```{r setup, include=FALSE}

#Make sure that "fully paid" is 1 in the factor variable
levels(lcdfTrn_AR$loan_status)
yTrn<-factor(if_else(lcdfTrn_AR$loan_status=="Fully Paid", '1', '0') )
xDTrn<-lcdfTrn_AR %>% select(-loan_status, -actualTerm, - actualReturn, -annRet,)


#running cross validation 
glmls_cv<- cv.glmnet(data.matrix(xDTrn), yTrn, family="binomial")

#experimenting with Ridge 
glmls_Ridge<- cv.glmnet(data.matrix(xDTrn), yTrn, family="binomial", alpha=0)
plot(glmls_Ridge)

#experimenting between Ridge and Lasso
glmls_Mid<- cv.glmnet(data.matrix(xDTrn), yTrn, family="binomial", alpha=.5)
plot(glmls_Mid)

#finding the minimum lambda
glmls_cv$lambda.min
#finding within 1 standard error
glmls_cv$lambda.1se
```

## Question 1B

For the linear model, what is the loss function, and link function you use ?

The Logit function

logit = 3.277 - 3.54(int_rate) - 8.297(grade) - 4.21(sub_grade) - 3.49(home_ownership) - 8.13(dti) - 2.24(acc_open_past_24mths) - 1.47(num_rev_tl_bal_gt_0) - 6.01(percent_bc_gt_75) + 6.01(tot_hi_cred_lim)

## Question 1C

Compare performance of models with that of random forests and gradient boosted tree models (which you developed in your last assignment).

Comparing the performance of linear models to that of Random Forest and gradient boosted tree models will generally underperform based on the nature of the model. But looking at our data for the xgboost we found the best iteration to have an auc of .671019. We also found the RF model to have a score of .79. The linear models were showing an auc score of .6935 for the lambda.min. Meaning that the linear model did a slightly better job.

```{r, output.lines=50}

#making predictions for Lambda min with Training and Test
glmPredls_Min=predict ( glmls_cv,data.matrix(xDTrn), s="lambda.min" ) 
head(glmPredls_Min)

#making predictions with probability for Lambda min Training 
glmPredls_pMin=predict(glmls_cv,data.matrix(xDTrn), s="lambda.min", type="response" )
head(glmPredls_pMin) 

#making predictions for Lambda.se
glmPredls_SE=predict ( glmls_cv,data.matrix(xDTrn), s="lambda.1se" ) 
head(glmPredls_SE)

#making predictions with probability for Lambda.1se
glmPredls_pSE=predict(glmls_cv,data.matrix(xDTrn), s="lambda.1se", type="response" )
head(glmPredls_pSE) 

#showing the auc values for using different lambda values.
predsauc <- prediction(glmPredls_pMin, lcdfTrn_AR$loan_status, label.ordering = c("Charged Off", "Fully Paid"))
aucPerf <- performance(predsauc, "auc")
aucPerf@y.values

#auc value for lambda.se
predsaucSE <- prediction(glmPredls_pSE, lcdfTrn_AR$loan_status, label.ordering = c("Charged Off", "Fully Paid"))
aucPerfSE <- performance(predsaucSE, "auc")
aucPerfSE@y.values
```

### Question 1D

Examine which variables are found to be important by the best models from the different methods, and comment on similarities, difference. What do you conclude?

Looking at the models it is very clear that interest rate and subgrade are some of the most important variables across the models. That is quite intuitive for models trying to predict actual return or for loan status as well. The Boosted model values that of interest rate and grades A and B very highly while the tree model values variables like the earliest credit line.

To analyze the variable importance of the boosting model we focused on the gain values as those are the contributions of each variable, based on the total gain from splits. It is also important to mention that the column cover is the proportion of examples covered by splits on each variable and frequency shows the relative frequency of times the variable has been used in trees.

```{r, output.lines=50}
#glm coefficients
tidy(coef(glmls_cv, s = glmls_cv$lambda.1se))
```

### Question 1E

In developing models above, do you find larger training samples to give better models ? Do you find balancing the training data examples across classes to give better models ?

When we balanced the data for our GLM model we found a slight decrease in the auc score. There was not a significant drop in performance though. When looking at Training size it was helpful to have a larger data set. This relates to later in looking at performance of models with only lower grade loans and how the performance drastically declined once many loans were taken out.

```{r, output.lines=50}
#to consider a more balanced data, we can include example weights
wts=if_else(yTrn==0, 1-sum(yTrn==0)/length(yTrn), 1-sum(yTrn==1)/length(yTrn) )
glmsw_cv<- cv.glmnet(data.matrix(xDTrn), yTrn, family = "binomial", weights= wts )
plot(glmsw_cv)

predsauc <- prediction(glmPredls_pMin, lcdfTrn_AR$loan_status, label.ordering =
                         c("Charged Off", "Fully Paid"))
aucPerf <- performance(predsauc, "auc")

```
